# Project 3 — AI Evaluator Examples

This project showcases how I think as an AI evaluator / rater when reviewing answers from large language models (LLMs).

## Start here (Evaluator Pack)

**Evaluator Pack (v1):** `./evaluator_pack/`

It includes:
- `rubric_v1.md` — scoring rubric (accuracy, completeness, clarity, safety, etc.)
- `taxonomy_v1.md` — error / issue taxonomy (hallucinations, omissions, tone, policy risk, etc.)
- `labeled_examples_v1.md` — labeled examples with scores + short justifications
- `evaluation_log.csv` — sample log showing consistent, structured annotations

## Narrative examples (short walkthrough)

`ai_evaluator_examples.md` — a few end-to-end examples (question → model answer → scoring → improved answer)

## What this demonstrates

- Careful, consistent judgment using rubrics
- Clear written justifications and actionable feedback
- Awareness of safety, tone, and user usefulness
- Comfort producing structured annotation artifacts (MD + CSV)
