# Project 3 — AI Evaluator Examples

Start with **Evaluator Pack (v1)** for a realistic workflow sample (rubric → taxonomy → labeled examples → log).  
Then skim **ai_evaluator_examples.md** for narrative end-to-end examples.

This project showcases how I think as an AI evaluator / rater when reviewing answers from large language models (LLMs).

## Start here (Evaluator Pack)

Evaluator Pack (v1): [./evaluator_pack/](./evaluator_pack/)

### 60-second skim (recommended)

1. Read: [labeled_examples_v1.md](./evaluator_pack/labeled_examples_v1.md) (best single artifact)
2. Skim: [taxonomy_v1.md](./evaluator_pack/taxonomy_v1.md) (how issues are tagged)
3. Glance: [evaluation_log.csv](./evaluator_pack/evaluation_log.csv) (what consistent logging looks like)
4. Optional: [rubric_v1.md](./evaluator_pack/rubric_v1.md) (scoring definitions)

It includes:

- [rubric_v1.md](./evaluator_pack/rubric_v1.md) — scoring rubric (accuracy, completeness, clarity, safety, etc.)
- [taxonomy_v1.md](./evaluator_pack/taxonomy_v1.md) — error / issue taxonomy (hallucinations, omissions, tone, policy risk, etc.)
- [labeled_examples_v1.md](./evaluator_pack/labeled_examples_v1.md) — labeled examples with scores + short justifications
- [evaluation_log.csv](./evaluator_pack/evaluation_log.csv) — sample log showing consistent, structured annotations

## Narrative examples (short walkthrough)

- [ai_evaluator_examples.md](./ai_evaluator_examples.md) — a few end-to-end examples (question → model answer → scoring → improved answer)

## What this demonstrates

- Careful, consistent judgment using rubrics
- Clear written justifications and actionable feedback
- Awareness of safety, tone, and user usefulness
- Comfort producing structured annotation artifacts (MD + CSV)
