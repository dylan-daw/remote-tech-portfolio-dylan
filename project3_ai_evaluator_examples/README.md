# Project 3 – AI Evaluator Examples

This project showcases how I think as an AI evaluator / rater when
reviewing answers from large language models (LLMs).

It includes:

- Example user questions
- Hypothetical model answers
- My scoring and reasoning using a simple rubric
- Suggested improved answers

The main file is:

- `ai_evaluator_examples.md` – concrete evaluation examples and a small
  rubric

This is meant to demonstrate:

- Careful, consistent judgment
- Ability to explain scores in clear language
- Awareness of safety, tone, and usefulness in model outputs
